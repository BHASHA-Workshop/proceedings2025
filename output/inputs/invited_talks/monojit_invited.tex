AI learns from data. Better data --- richer, cleaner, more diverse --- undeniably yields better models and evaluations. This narrative is familiar, almost axiomatic. Yet, these data-driven scaling approaches face two fundamental challenges. First, no corpus, however vast, can capture the infinite variability of human languages, contexts, and preferences. Second, every act of data creation is also an act of omission; each dataset is a boundary between inclusion and exclusion.

A sustainable path forward cannot simply be the endless accumulation of data, but rather the cultivation of models that can learn from less, adapt on the fly, and transfer understanding across contexts. Evaluation, too, must evolve from assessing isolated competencies to probing a modelâ€™s capacity for learning and adaptation in novel scenarios.

In this talk, I explore these ideas through the lens of culture. It is nearly impossible to define and capture the endless variations of cultures through datasets. I argue that AI models therefore must be trained for \emph{meta-cultural competency}---the ability to serve in any culture rather than a specific pre-defined culture. I also present novel methodologies for evaluating meta-cultural awareness.
