AI evaluation often claims ``multilingual'' coverage but relies on English-centric benchmarks that miss cultural and linguistic realities. To build truly inclusive systems, we need communities---not just datasets---in the loop. This keynote highlights why participatory evaluation matters: co-defining goals with local stakeholders, creating culturally grounded scenarios beyond translation, and combining human insight with scalable tools.

Drawing on initiatives like \textit{Samiksha} and \textit{DOSA}, this talk demonstrates how community-driven approaches uncover hidden biases, improve trust, and align AI with lived experiences of the Global Majority.

The talk concludes with practical models for collaboration between researchers, industry, and civil society to make evaluation democratic, accountable, and impactful.
